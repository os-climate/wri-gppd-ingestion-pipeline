{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41652f59-1798-4431-90dc-592dd4f64a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load WRI Power Plant data from 2019 dataset (see https://datasets.wri.org/dataset/globalpowerplantdatabase) for original source\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### We have a local copy rooted in the S3_BUCKET : WRI/global_power_plant_database_v_1_3/global_power_plant_database.csv\n",
    "### To tidy the data we factor into three tables:\n",
    "\n",
    "* **wri_plants** (all the fixed data about each plant)\n",
    "* **wri_annual_gwh** (per plant/per year annual generation in GWh)\n",
    "* **wri_estimated_gwh** (per plant/per year estimated generation in GWh)\n",
    "\n",
    "### The next step is to enrich with OS-C Factor metadata\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92377eb7-1d1b-4662-ac08-99877153832b",
   "metadata": {},
   "source": [
    "Load Credentials and Data Commons libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18bf3b-80d7-4b25-8ae4-9273709a0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80f217f8-d7c6-40fd-8552-30a98dc4481d",
   "metadata": {},
   "source": [
    "# Thanks to https://stackoverflow.com/a/56172803/1291237 (and their CC-BY-SA 4.0 contribution!)\n",
    "def set_metadata(tbl, col_meta={}, tbl_meta={}):\n",
    "    \"\"\"Store table- and column-level metadata as json-encoded byte strings.\n",
    "\n",
    "    Table-level metadata is stored in the table's schema.\n",
    "    Column-level metadata is stored in the table columns' fields.\n",
    "\n",
    "    To update the metadata, first new fields are created for all columns.\n",
    "    Next a schema is created using the new fields and updated table metadata.\n",
    "    Finally a new table is created by replacing the old one's schema, but\n",
    "    without copying any data.\n",
    "\n",
    "    Args:\n",
    "        tbl (pyarrow.Table): The table to store metadata in\n",
    "        col_meta: A json-serializable dictionary with column metadata in the form\n",
    "            {\n",
    "                'column_1': {'some': 'data', 'value': 1},\n",
    "                'column_2': {'more': 'stuff', 'values': [1,2,3]}\n",
    "            }\n",
    "        tbl_meta: A json-serializable dictionary with table-level metadata.\n",
    "    \"\"\"\n",
    "    # Create updated column fields with new metadata\n",
    "    if col_meta or tbl_meta:\n",
    "        fields = []\n",
    "        for col in tbl.schema.names:\n",
    "            if col in col_meta:\n",
    "                # Get updated column metadata\n",
    "                metadata = tbl.field(col).metadata or {}\n",
    "                for k, v in col_meta[col].items():\n",
    "                    metadata[k] = json.dumps(v).encode('utf-8')\n",
    "                # Update field with updated metadata\n",
    "                fields.append(tbl.field(col).with_metadata(metadata))\n",
    "            else:\n",
    "                fields.append(tbl.field(col))\n",
    "        \n",
    "        # Get updated table metadata\n",
    "        tbl_metadata = tbl.schema.metadata or {}\n",
    "        for k, v in tbl_meta.items():\n",
    "            if type(v)==bytes:\n",
    "                tbl_metadata[k] = v\n",
    "            else:\n",
    "                tbl_metadata[k] = json.dumps(v).encode('utf-8')\n",
    "\n",
    "        # Create new schema with updated field metadata and updated table metadata\n",
    "        schema = pa.schema(fields, metadata=tbl_metadata)\n",
    "\n",
    "        # With updated schema build new table (shouldn't copy data)\n",
    "        # tbl = pa.Table.from_batches(tbl.to_batches(), schema)\n",
    "        tbl = tbl.cast(schema)\n",
    "\n",
    "    return tbl\n",
    "\n",
    "\n",
    "def decode_metadata(metadata):\n",
    "    \"\"\"Arrow stores metadata keys and values as bytes.\n",
    "    We store \"arbitrary\" data as json-encoded strings (utf-8),\n",
    "    which are here decoded into normal dict.\n",
    "    \"\"\"\n",
    "    if not metadata:\n",
    "        # None or {} are not decoded\n",
    "        return metadata\n",
    "\n",
    "    decoded = {}\n",
    "    for k, v in metadata.items():\n",
    "        key = k.decode('utf-8')\n",
    "        val = json.loads(v.decode('utf-8'))\n",
    "        decoded[key] = val\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def table_metadata(tbl):\n",
    "    \"\"\"Get table metadata as dict.\"\"\"\n",
    "    return decode_metadata(tbl.schema.metadata)\n",
    "\n",
    "\n",
    "def column_metadata(tbl):\n",
    "    \"\"\"Get column metadata as dict.\"\"\"\n",
    "    return {col.name: decode_metadata(col.metadata) for col in tbl.schema}\n",
    "\n",
    "\n",
    "def get_metadata(tbl):\n",
    "    \"\"\"Get column and table metadata as dicts.\"\"\"\n",
    "    return column_metadata(tbl), table_metadata(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17be530-72e4-4914-9fbd-0af2609b9688",
   "metadata": {},
   "source": [
    "Build a map and define schema mapping logic for parquet to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cec97c-f230-4ae2-a8e1-99f7fa346c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "_wsdedup = re.compile(r\"\\s+\")\n",
    "_usdedup = re.compile(r\"__+\")\n",
    "_rmpunc = re.compile(r\"[,.()&$/+-]+\")\n",
    "# 63 seems to be a common max column name length\n",
    "def snakify(name, maxlen=63):\n",
    "    if isinstance(name, list):\n",
    "        return [snakify(e) for e in name]\n",
    "    w = str(name).casefold().rstrip().lstrip()\n",
    "    w = w.replace(\"-\", \"_\")\n",
    "    w = _rmpunc.sub(\"\", w)\n",
    "    w = _wsdedup.sub(\"_\", w)\n",
    "    w = _usdedup.sub(\"_\", w)\n",
    "    w = w.replace(\"average\", \"avg\")\n",
    "    w = w.replace(\"maximum\", \"max\")\n",
    "    w = w.replace(\"minimum\", \"min\")\n",
    "    w = w.replace(\"absolute\", \"abs\")\n",
    "    w = w.replace(\"source\", \"src\")\n",
    "    w = w.replace(\"distribution\", \"dist\")\n",
    "    # these are common in the sample names but unsure of standard abbv\n",
    "    #w = w.replace(\"inference\", \"inf\")\n",
    "    #w = w.replace(\"emissions\", \"emis\")\n",
    "    #w = w.replace(\"intensity\", \"int\")\n",
    "    #w = w.replace(\"reported\", \"rep\")\n",
    "    #w = w.replace(\"revenue\", \"rev\")\n",
    "    w = w[:maxlen] \n",
    "    return w\n",
    "\n",
    "def snakify_columns(df, inplace=False, maxlen=63):\n",
    "    icols = df.columns.to_list()\n",
    "    ocols = snakify(icols, maxlen=maxlen)\n",
    "    scols = set(ocols)\n",
    "    if (len(set(ocols)) < len(ocols)):\n",
    "        raise ValueError(\"remapped column names were not unique!\")\n",
    "    rename_map = dict(list(zip(icols,snakify(icols))))\n",
    "    return df.rename(columns=rename_map, inplace=inplace)\n",
    "\n",
    "rename_year_columns={}\n",
    "for y in range(1900,2100):\n",
    "    rename_year_columns[str(y)] = 'y{yr}'.format(yr=y)\n",
    "#rename_year_columns\n",
    "\n",
    "_p2smap = {\n",
    "    'object': 'varchar',\n",
    "    'string': 'varchar',\n",
    "    'str': 'varchar',\n",
    "    'float32': 'real',\n",
    "    'Float32': 'real',\n",
    "    'float64': 'double',\n",
    "    'Float64': 'double',\n",
    "    'int32': 'integer',\n",
    "    'Int32': 'integer',\n",
    "    'int64': 'bigint',\n",
    "    'Int64': 'bigint',\n",
    "    'category': 'varchar',\n",
    "    'datetime64[ns, UTC]': 'timestamp',\n",
    "    'datetime64[ns]': 'timestamp'\n",
    "}\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def generate_table_schema_pairs(df):\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0],t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72b432-76cc-41b9-be13-f90f89f05107",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527df23-a9b2-4084-93aa-26ae6f3685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_resource.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d68d14-7d11-4ec8-837f-14a1d6f4f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3be756-8a5e-4b7f-97d7-724244e99975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c467dc-abb1-4efa-aefb-f631a5774bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'wri_gppd_md'  # Add the _md so we don't disturb others who are trying to use a \"stable\" version of wri_gppd right now\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26e42c-93bd-4310-9534-cf37e9073dc3",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a82e7-cd97-4dc5-a661-839f0513ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = generate_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c9840-1f07-4d3c-ab34-ca401dc543e5",
   "metadata": {},
   "source": [
    "Load WRI data file using pandas *read_csv* and using *ingest_uuid* as the global UUID for this ingestion\n",
    "\n",
    "# Do we really want to add UUID to every row, or better to put into table-level metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d64dc-7928-4bcf-b919-9a7a4d08eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "wri_file = bucket.Object('WRI/global_power_plant_database_v_1_3/global_power_plant_database.csv').get()['Body']\n",
    "\n",
    "# Because NaN cannot be converted to int type, we cannot use int as a datatype for years that are NaN\n",
    "\n",
    "wri_df = pd.read_csv(wri_file, dtype={'latitude':'float64', 'longitude':'float64', 'capacity_mw':'float64', 'other_fuel3':'str'})\n",
    "\n",
    "display(wri_df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1e8ec08-28b0-4bdc-8137-0be965ec749c",
   "metadata": {},
   "source": [
    "# Add a unique identifier to the data set.\n",
    "uid = str(uuid.uuid4())\n",
    "wri_df['uuid'] = uid\n",
    "\n",
    "display(wri_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebc560-a859-4601-bc67-bbe1f8a0ff2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Melt the generation data into a more tidy format, dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4eb25-d1a7-4bab-bf26-12c4873666b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wri_plants = wri_df[['country', 'country_long', 'name', 'gppd_idnr', 'capacity_mw',\n",
    "       'latitude', 'longitude', 'primary_fuel', 'other_fuel1', 'other_fuel2',\n",
    "       'other_fuel3', 'commissioning_year', 'owner', 'source', 'url',\n",
    "       'geolocation_source', 'wepp_id', 'year_of_capacity_data', 'generation_data_source',\n",
    "#        'uuid'\n",
    "    ]]\n",
    "\n",
    "# wri_id_vars = ['gppd_idnr', 'uuid']\n",
    "wri_id_vars = ['gppd_idnr']\n",
    "\n",
    "wri_value_vars = ['generation_gwh_2013', 'generation_gwh_2014', 'generation_gwh_2015', 'generation_gwh_2016',\n",
    "                  'generation_gwh_2017', 'generation_gwh_2018', 'generation_gwh_2019']\n",
    "wri_annual_gwh = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='generation_gwh')\n",
    "wri_annual_gwh['year'] = pd.to_numeric(wri_annual_gwh['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "wri_annual_gwh.dropna(subset=['generation_gwh'],inplace=True)\n",
    "\n",
    "# Push uuid to the end of the column list\n",
    "# wri_annual_gwh.insert(len(wri_annual_gwh.columns)-1, 'uuid', wri_annual_gwh.pop('uuid'))\n",
    "\n",
    "# display(wri_annual_gwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd331e-0014-44c1-8d5a-5434140f0a4d",
   "metadata": {},
   "source": [
    "### Create and store metadata for annual_gwh table (1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab110c5-5f09-4e8b-adc8-08fe521392ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_fields = {\n",
    "    'year': { 'description': 'year of report', 'dimension': 'year'},\n",
    "    'gppd_idnr': { 'description': 'unique index into plants table', 'dimension': None},\n",
    "    'generation_gwh': { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "}\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'WRI GPPD Annual Generation Data (GWh)',\n",
    "    'description': 'A tidy representation of annual generation data from the WRI GPPD database',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "    # How should we describe our transformative step here?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84f129-1901-4c67-b5e9-a996169a8495",
   "metadata": {},
   "source": [
    "Construct the combined metadata by merging existing table metadata and custom metadata.\n",
    "Note: The metadata content must be JSON serialisable and encoded as bytes; the metadata key must also be encoded as bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34490f08-dc88-43e5-87fa-7415013a1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'annual_gwh'\n",
    "create_trino_pipeline (s3, schemaname, tablename, wri_annual_gwh, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4116cf3-a06c-4224-8e6c-b2943f15cefb",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958517a-a986-4c3a-bd01-e16d1717d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a60048-3709-4255-92ea-cb6384fcd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3a4d9-0798-4a83-8eaa-f349a4821b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09430a-8e4c-4709-9aef-0e171c1c53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_meta_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dae76e-dda6-4469-9797-8a3fbf6e2446",
   "metadata": {},
   "source": [
    "Merge the estimation data so that estimates and notes are 1:1 together, dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af6089-c252-4ca9-9915-9c2613e77496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wri_id_vars = ['gppd_idnr', 'uuid']\n",
    "wri_id_vars = ['gppd_idnr']\n",
    "wri_value_vars = ['estimated_generation_gwh_2013', 'estimated_generation_gwh_2014', 'estimated_generation_gwh_2015',\n",
    "                  'estimated_generation_gwh_2016', 'estimated_generation_gwh_2017']\n",
    "wri_estimated_gwh = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='estimated_generation_gwh')\n",
    "wri_estimated_gwh['year'] = pd.to_numeric(wri_estimated_gwh['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "\n",
    "wri_value_vars = ['estimated_generation_note_2013', 'estimated_generation_note_2014', 'estimated_generation_note_2015',\n",
    "                  'estimated_generation_note_2016', 'estimated_generation_note_2017']\n",
    "wri_estimated_note = wri_df.melt(wri_id_vars, wri_value_vars, var_name='year', value_name='estimated_generation_note')\n",
    "wri_estimated_note['year'] = pd.to_numeric(wri_estimated_note['year'].apply(lambda x: int(x.split('_')[-1])))\n",
    "\n",
    "# Push uuid to the end of the column list\n",
    "# wri_estimated_gwh.insert(len(wri_estimated_gwh.columns)-1, 'uuid', wri_estimated_gwh.pop('uuid'))\n",
    "\n",
    "display(wri_estimated_gwh)\n",
    "\n",
    "# wri_estimated_gwh = wri_estimated_gwh.merge(wri_estimated_note, on=['gppd_idnr', 'year', 'uuid'], validate=\"one_to_one\")\n",
    "wri_estimated_gwh = wri_estimated_gwh.merge(wri_estimated_note, on=['gppd_idnr', 'year'], validate=\"one_to_one\")\n",
    "\n",
    "wri_estimated_gwh.dropna(subset=['estimated_generation_gwh'],inplace=True)\n",
    "\n",
    "# Push uuid to the end of the column list\n",
    "# wri_estimated_gwh.insert(len(wri_estimated_gwh.columns)-1, 'uuid', wri_estimated_gwh.pop('uuid'))\n",
    "\n",
    "display(wri_estimated_gwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b616a7-7797-459f-b726-caf4d450e4e5",
   "metadata": {},
   "source": [
    "### Create and store metadata for estimated_gwh table\n",
    "\n",
    "Convert the DataFrame to an Arrow table using PyArrow and inspect the table’s metadata property "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae8f86-bdc3-4916-9ed6-8e2d0a845c2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230459b0-5bf8-4f39-9325-1cd2808ef6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_meta_fields = {\n",
    "    'year': { 'description': 'year of report', 'dimension': 'year'},\n",
    "    'gppd_idnr': { 'description': 'unique index into plants table', 'dimension': None},\n",
    "    'estimated_generation_gwh': { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'},\n",
    "    'estimated_generation_note': { 'description': 'type of generation estimated', 'dimension': None}\n",
    "}\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'WRI GPPD Estimated Generation Data (GWh)',\n",
    "    'description': 'A tidy representation of estimated generation data from the WRI GPPD database',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "    # How should we describe our transformative step here?\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7d9f8-66fa-4520-b7ee-abb26757a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'estimated_gwh'\n",
    "create_trino_pipeline (s3, schemaname, tablename, wri_estimated_gwh, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfb5fe-74b6-4aea-b4d2-982c1680877d",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06282946-f510-49f8-a05f-db287bab6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{tname}.parquet'.format(sname=schemaname,tname=tablename,uuid=ingest_uuid)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b59702-ba1a-4d7d-bb8b-812a67a5c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb13c5-2a6c-4ab3-87ae-fa84ee0861c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07193faf-63a5-4afa-89f3-de6e2a8c85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_meta_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7a9f6-6603-4a21-9edd-fc3fc19dcef2",
   "metadata": {},
   "source": [
    "### Finally, write out plants with metadata\n",
    "\n",
    "Create the actual metadata for the source.  In this case, it is WRI_GPPD.\n",
    "The quoted text comes from the README.txt file that comes with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6a5ed-1ee1-46dd-b877-41dd1d139f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {}\n",
    "metadata_text = \"\"\"Title: Global Power Plant Database\n",
    "Description: A comprehensive, global, open source database of power plants\n",
    "Version: 1.3.0\n",
    "Release Date: 2021-06-02\n",
    "URI: http://datasets.wri.org/dataset/globalpowerplantdatabase\n",
    "Copyright: Copyright 2018-2021 World Resources Institute and Data Contributors\n",
    "License: Creative Commons Attribution 4.0 International -- CC BY 4.0\n",
    "Contact: powerexplorer@wri.org\n",
    "Citation: Global Energy Observatory, Google, KTH Royal Institute of Technology in Stockholm, Enipedia, World Resources Institute. 2019. Global Power Plant Database. Published on Resource Watch and Google Earth Engine. http://resourcewatch.org/ https://earthengine.google.com/  \"\"\"\n",
    "\n",
    "for line in metadata_text.split('\\n'):\n",
    "    k, v = line.split(':', 1)\n",
    "    k = snakify(k)\n",
    "    custom_meta_content[k] = v\n",
    "\n",
    "custom_meta_content['abstract'] = \"\"\"An affordable, reliable, and environmentally sustainable power sector is central to modern society.\n",
    "Governments, utilities, and companies make decisions that both affect and depend on the power sector.\n",
    "For example, if governments apply a carbon price to electricity generation, it changes how plants run and which plants are built over time.\n",
    "On the other hand, each new plant affects the electricity generation mix, the reliability of the system, and system emissions.\n",
    "Plants also have significant impact on climate change, through carbon dioxide (CO2) emissions; on water stress, through water withdrawal and consumption; and on air quality, through sulfur oxides (SOx), nitrogen oxides (NOx), and particulate matter (PM) emissions.\n",
    "\n",
    "The Global Power Plant Database is an open-source open-access dataset of grid-scale (1 MW and greater) electricity generating facilities operating across the world.\n",
    "\n",
    "The Database currently contains nearly 35000 power plants in 167 countries, representing about 72% of the world's capacity.\n",
    "Entries are at the facility level only, generally defined as a single transmission grid connection point.\n",
    "Generation unit-level information is not currently available. \n",
    "The methodology for the dataset creation is given in the World Resources Institute publication \"A Global Database of Power Plants\" [0].\n",
    "Associated code for the creation of the dataset can be found on GitHub [1].\n",
    "See also the technical note published in early 2020 on an improved methodology to estimate annual generation [2].\n",
    "\n",
    "To stay updated with news about the project and future database releases, please sign up for our newsletter for the release announcement [3].\n",
    "\n",
    "\n",
    "[0] www.wri.org/publication/global-power-plant-database\n",
    "[1] https://github.com/wri/global-power-plant-database\n",
    "[2] https://www.wri.org/publication/estimating-power-plant-generation-global-power-plant-database\n",
    "[3] https://goo.gl/ivTvkd\"\"\"\n",
    "custom_meta_content['name'] = 'WRI_GPPD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9d93a-71c6-43b0-b9bc-ff7505b070ee",
   "metadata": {},
   "source": [
    "Create the metadata for all the fields in all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c407133-8e2f-4a27-9f88-5ff9cf197e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_text = \"\"\"`country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [https://www.iso.org/iso-3166-country-codes.html]\n",
    "`country_long` (text): longer form of the country designation\n",
    "`name` (text): name or title of the power plant, generally in Romanized form\n",
    "`gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "`capacity_mw` (number): electrical generating capacity in megawatts\n",
    "`latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "`other_fuel1` (text): energy source used in electricity generation or export\n",
    "`other_fuel2` (text): energy source used in electricity generation or export\n",
    "`other_fuel3` (text): energy source used in electricity generation or export\n",
    "`commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "`owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "`source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "`url` (text): web document corresponding to the `source` field\n",
    "`geolocation_source` (text): attribution for geolocation information\n",
    "`wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "`year_of_capacity_data` (number): year the capacity information was reported\n",
    "`generation_data_source` (text): attribution for the reported generation information\"\"\"\n",
    "\n",
    "field_descs = [line.split(': ')[1] for line in field_text.split('\\n')]\n",
    "field_keys = [line.split(': ')[0].split(' ')[0][1:-1] for line in field_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd3141-b0f0-4c04-b8f1-4dcbdbebd482",
   "metadata": {},
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbed61-7a9a-45c2-91ba-27d33ee677be",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_meta_fields = {}\n",
    "for k, v in zip(field_keys, field_descs):\n",
    "    custom_meta_fields[k] = { 'description': v }\n",
    "\n",
    "custom_meta_fields['capacity_mw']['dimension'] = 'MW'\n",
    "custom_meta_fields['latitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['longitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['commissioning_year']['dimension'] = 'year'\n",
    "custom_meta_fields['year_of_capacity_data']['dimension'] = 'year'\n",
    "custom_meta_fields['year'] = { 'description': 'year of report', 'dimension': 'year'}\n",
    "custom_meta_fields['gppd_idnr'] = { 'description': 'unique index into plants table', 'dimension': None}\n",
    "custom_meta_fields['generation_gwh'] = { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_gwh'] = { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_note'] = { 'description': 'label of the model/method used to estimate generation for the year', 'dimension': None }\n",
    "custom_meta_key_fields = 'metafields'\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'Global Power Plant Database',\n",
    "    'description': 'A comprehensive, global, open source database of power plants',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "}\n",
    "custom_meta_key = 'metaset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b19d3-f756-4081-bf99-324e401f0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'plants'\n",
    "create_trino_pipeline (s3, schemaname, tablename, wri_plants, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7b596-8400-4df8-bb05-9c1a313dbb88",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643e7b0-0b2f-4187-a3cf-c3fa715715e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{tname}.parquet'.format(sname=schemaname,tname=tablename,uuid=ingest_uuid)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4046b-bc58-4363-963c-b61b0f6015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f423647-23b0-44b4-860a-892878519261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05756f41-374f-4010-8924-1d3293c30f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_meta_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39786-15d8-430f-9111-b05fc08e73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wri_plants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5b1e6-17a4-4401-a82b-c998feee4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb2656-b4e4-4693-a6a0-b76018282398",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d557ebe-1c26-4129-90b3-ff774f4f87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'wri_gppd_md',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':uid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec686661-b22b-449d-9584-66b95bef9989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
